{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a787a1d3-c710-4c94-853f-3b5585b18f12",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "Bayes' Theorem states that the conditional probability of an event, based on the occurrence of another event, is equal to the likelihood of the second event given the first event multiplied by the probability of the first event.\n",
    "\n",
    "Bayes’ theorem describes the probability of occurrence of an event related to any condition. It is also considered for the case of conditional probability. Bayes theorem is also known as the formula for the probability of “causes”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63655d1-fe5a-4e89-8942-007a80ac7084",
   "metadata": {},
   "source": [
    "# ANSWER 2\n",
    "P(A|B) = [P(B|A) P(A)] / P(B)\n",
    "\n",
    "where:\n",
    "1. P(A|B) is the posterior probability of event A given event B .\n",
    "2. P(B|A) is the likelihood of event B given event A .\n",
    "3. P(A) is the prior probability of event A .\n",
    "4. P(B) is the probability of event B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae94c3-9d89-4a3e-812c-28a3fbfd999f",
   "metadata": {},
   "source": [
    "# ANSWER 3\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics, and it finds application in various fields. It allows us to update our beliefs or probabilities based on new evidence. In practice, Bayes' theorem is used in a wide range of applications, some of which include:\n",
    "\n",
    "1. Medical Diagnosis: Bayes' theorem is used in medical diagnostics to update the probability of a patient having a certain condition based on the results of tests and other relevant information.\n",
    "2. Spam Filtering: In email spam filtering, Bayes' theorem can be used to determine the probability that an incoming email is spam based on the words or patterns found in the email's content.\n",
    "3. Machine Learning and AI: Bayes' theorem forms the basis for various probabilistic models, such as Naive Bayes classifiers, which are used in text classification, sentiment analysis, and other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcb7b3-a4f6-4131-ab08-e0fe855d8b33",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "## Conditional Probability:\n",
    "Conditional probability is a measure of the probability of an event occurring given that another event has already occurred. Mathematically, the conditional probability of event A given event B is denoted as P(A|B) and is calculated as:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "where:\n",
    "1. P(A|B) is the conditional probability of event A given event B.\n",
    "2. P(A ∩ B) is the probability of both events A and B occurring simultaneously (the intersection of A and B).\n",
    "3. P(B) is the probability of event B occurring.\n",
    "\n",
    "## Bayes' Theorem:\n",
    "Bayes' theorem allows us to update the probability of an event based on new evidence. It is derived from the principles of conditional probability. Mathematically, Bayes' theorem is formulated as follows:\n",
    "\n",
    "P(A|B) = [P(B|A) P(A)] / P(B)\n",
    "\n",
    "where:\n",
    "1. P(A|B) is the posterior probability of event A given event B .\n",
    "2. P(B|A) is the likelihood of event B given event A .\n",
    "3. P(A) is the prior probability of event A .\n",
    "4. P(B) is the probability of event B.\n",
    "\n",
    "## Relationship between Bayes' Theorem and Conditional Probability:\n",
    "\n",
    "The formula of Bayes' theorem,involves both conditional probability and prior probabilities. It shows how the probability of A given B can be calculated based on the likelihood of B given A and the prior probability of A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0667a92-1fcd-4164-a3ea-29442aa93715",
   "metadata": {},
   "source": [
    "# ANSWER 5\n",
    "Choosing the appropriate type of Naive Bayes classifier for any given problem depends on the nature of the data and the assumptions we can make about the independence of features. There are three common types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n",
    "1. Gaussian Naive Bayes:\n",
    "\n",
    "--> Assumption: Assumes that the features follow a Gaussian (normal) distribution.\n",
    "\n",
    "--> Suitable for: Continuous numerical features or features that can be reasonably approximated by a Gaussian distribution.\n",
    "\n",
    "--> Use case: It is commonly used for classification problems where the features have continuous values, such as in natural language processing tasks when using \n",
    "numerical features like word frequencies or sentiment scores.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "\n",
    "--> Assumption: Assumes that the features are discrete and follow a multinomial distribution.\n",
    "\n",
    "--> Suitable for: Features represented as word counts or frequencies (e.g., in text classification problems using the bag-of-words model).\n",
    "\n",
    "--> Use case: Often applied in natural language processing tasks, like document classification, spam filtering, and sentiment analysis, where the input features are discrete and represent word occurrences or frequencies.\n",
    "\n",
    "3. Bernoulli Naive Bayes:\n",
    "\n",
    "--> Assumption: Assumes that the features are binary (i.e., presence or absence of a feature).\n",
    "\n",
    "--> Suitable for: Binary features or features that can be represented as binary values.\n",
    "\n",
    "--> Use case: Appropriate when dealing with binary data, such as document classification where the features represent the presence or absence of specific words or features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24797a43-7ba9-49cd-a0f5-f9ceea467959",
   "metadata": {},
   "source": [
    "# ANSWER 6 \n",
    "## FOR CLASS A :\n",
    "\n",
    "P(X1=3|A) = 0.4\n",
    "\n",
    "P(X2=4|A) = 0.333\n",
    "## FOR CLASS B :\n",
    "\n",
    "P(X1=3|B) = 0.2\n",
    "\n",
    "P(X2=4|B) = 0.43\n",
    "\n",
    "P(X1=3) = 0.33\n",
    "\n",
    "P(X2=4) = 0.375\n",
    "\n",
    "P(X1=3,X2=4) = 0.125\n",
    "\n",
    "## FOR CLASS A :\n",
    "\n",
    "P(A|X1 =3 , X2 = 4) = 0.017\n",
    "\n",
    "## FOR CLASS B :\n",
    "\n",
    "P(B|X1 =3 , X2 = 4) = 0.0107\n",
    "\n",
    "Compare the two posterior probabilities and select the class with the higher probability. In this case, P(A∣X1=3,X2=4) is greater than P(B∣X1=3,X2=4), so Naive Bayes would predict the new instance to belong to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c1575-8854-4c9d-8f70-a99bf2684777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
